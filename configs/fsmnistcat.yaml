# logger options
image_save_iter: 500        # How often do you want to save output images during training
image_display_iter: 500       # How often do you want to display output images during training
display_size: 10             # How many images do you want to display each time
snapshot_save_iter: 80000     # How often do you want to save trained models
log_iter: 250                  # How often do you want to log the training stats
experiment_id: _qvae12 # _qvae09 #'vae_20'
mode: InfoCatVAE #[InfoCatVAE | CatVAE | CatVAEInfoEvaluation] 

# optimization options
max_iter: 150000             # maximum number of training iterations
batch_size: 124                # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialiget_schedulerzation [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: constant #step               # [step/constant] learning rate scheduler
step_size: 140000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate

vae_adv_full: True           # propagate gan signal to encoder
use_gen_info: False          # maximize mutual information for generated samples

# loss terms coeff
kl_w:  0.005 #0.0005 #0.005 #0.005!!! #.001
inf_w: 0.01

inf_w_catg: 0.0005
inf_w_cont: 0.0005

#adv_w: 0.001 #0.1, 0.005
kl_ctg_w: 0.9 #used only in TrainerInfoCatVAE

# latent options
latent:
  latent_dim: 32 #10 #64#ingnored in cvae
  inform_dim: 0 # dim of continious latent variable to be maximized
  categorical:  10
  continious:   16

temperature:
  start: 1.0  
  minim: 0.3
  ann_rate: 0.000015
  iter_dec: 1500 # after how many iterations decrease temperature
  
# VAE model options
gen:
  dim: 64                    # number of filters in the bottommost layer
  mlp_dim: 512                 # number of filters in MLP
  n_mlp: 2
  activ: lrelu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 0                    # number of residual blocks in content encoder/decoder
  pad_type: zero           # padding type [zero/reflect]
  norm: bn
dis:
  dim: 64                    # number of filters in the bottommost layer
  n_downsample: 2                  # number of layers in D
  n_res: 0                    # number of residual blocks in content encoder/decoder
  norm: bn                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero           # padding type [zero/reflect]
  gan_type: nsgan #nsgan             # GAN loss [lsgan/vanilla]
  num_scales: 3               # number of scales
qnet: #Q(c|x_)  
  dim: 64                    # number of filters in the bottommost layer
  n_downsample: 2                  # number of layers in D
  n_res: 0                    # number of residual blocks in content encoder/decoder
  norm: bn                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero           # padding type [zero/reflect]

# data options
image:
  image_dim: 1                              # number of image channels [1/3]
  image_size: 28 # first resize the shortest image side to this size

num_workers: 8                              # number of data loading threads
data: fsmnist
data_root: '~/projects/datasets/fsmnist'  # dataset folder location