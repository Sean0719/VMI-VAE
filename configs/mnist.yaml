# logger options
image_save_iter: 500        # How often do you want to save output images during training
image_display_iter: 500       # How often do you want to display output images during training
display_size: 10             # How many images do you want to display each time
snapshot_save_iter: 10000     # How often do you want to save trained models
log_iter: 250                  # How often do you want to log the training stats
experiment_id: '_qvae03' #'_info09' #'gan_20' #'10' etc
mode: InfoVAE #[VAE/VAEGAN/InfoVAEGAN/InfoVAE]

# optimization options
max_iter: 150000             # maximum number of training iterations
batch_size: 124                # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialiget_schedulerzation [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: constant #step               # [step/constant] learning rate scheduler
step_size: 140000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate

vae_adv_full: True           # propagate gan signal to encoder

# loss terms coef
kl_w:  0.005 #
adv_w: 0.001 #
inf_w: 0.0005

# latent options
latent:
  latent_dim: 32 #10 #64
  inform_dim: 2
  categorical:  10
  continious:   16

temperature:
  start: 1.0  
  minim: 0.5
  ann_rate: 0.000015
  iter_dec: 1000 # after how many iterations decrease temperature
  
# VAE model options
gen:
  dim: 64                    # number of filters in the bottommost layer
  mlp_dim: 512                 # number of filters in MLP
  n_mlp: 2
  activ: lrelu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 0                    # number of residual blocks in content encoder/decoder
  pad_type: zero           # padding type [zero/reflect]
  norm: bn
dis:
  dim: 64                    # number of filters in the bottommost layer
  n_downsample: 2                  # number of layers in D
  n_res: 0                    # number of residual blocks in content encoder/decoder
  norm: bn                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero           # padding type [zero/reflect]
  gan_type: nsgan #nsgan             # GAN loss [lsgan/vanilla]
  num_scales: 3               # number of scales

#Q(c|x) auxiliary distribution network parameters 
qnet:
  dim: 64                    # number of filters in the bottommost layer
  n_downsample: 2                  # number of layers in D
  n_res: 0                    # number of residual blocks in content encoder/decoder
  norm: bn                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero           # padding type [zero/reflect]

# data options
image:
  image_dim: 1                              # number of image channels [1/3]
  image_size: 28 # first resize the shortest image side to this size

num_workers: 8                              # number of data loading threads
data_root: '~/projects/datasets/mnist'  # dataset folder location